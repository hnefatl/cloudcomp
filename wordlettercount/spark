#! /bin/bash
MAIN_FILE=wordlettercount.py
mapfile -t AWS_CREDS < <(awk '/ = / {print $NF}' ~/.aws/credentials)

function _build() {
    docker build . -t wordlettercount
    docker tag wordlettercount:latest clgroup8/wordlettercount
    docker push clgroup8/wordlettercount
}

function _kube_clean() {
    kubectl delete po --all
}

function _kube() {
    _kube_clean
    _build
    KUBE_MASTER=$(kubectl cluster-info | grep "master" | grep -o 'https://[^"]*.com')

    # Jars not needed on master because docker file puts them in imported folder (I think...)
    spark-submit \
        --master "k8s://$KUBE_MASTER" \
        --deploy-mode cluster \
        --name simpleapp \
        --conf spark.executor.instances=2 \
        --conf spark.kubernetes.pyspark.pythonVersion=3 \
        --conf spark.kubernetes.container.image=docker.io/clgroup8/wordlettercount:latest \
        --conf spark.kubernetes.container.image.pullPolicy=Always \
        "file:///usr/spark-2.4.0/work-dir/$MAIN_FILE" \
        "${AWS_CREDS[@]}" \
        "$@"
}

function _local() {
    spark-submit \
        --jars aws-java-sdk-1.7.4.jar,hadoop-aws-2.7.0.jar \
        "$MAIN_FILE" \
        "${AWS_CREDS[@]}" \
        "$@"
}

case $1 in
    kubernetes|k8s)
        shift
        _kube "$@"
        ;;
    local|l)
        shift
        _local "$@"
        ;;
    *)
        echo "Usage: spark [kubernetes|local] RUN_ARGS"
        echo "kubernetes - Launch in kubernetes"
        echo "local - Launch locally"
        exit 1
esac
